<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Infra | Devex Tech Blog]]></title>
  <link href="http://devex.github.io/blog/categories/infra/atom.xml" rel="self"/>
  <link href="http://devex.github.io/"/>
  <updated>2015-07-16T08:41:32-04:00</updated>
  <id>http://devex.github.io/</id>
  <author>
    <name><![CDATA[Devex]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[How to Upgrade When Stuck With Unsupported Ubuntu Releases]]></title>
    <link href="http://devex.github.io/blog/2015/02/16/ubuntu-eol-upgrades/"/>
    <updated>2015-02-16T10:02:50-05:00</updated>
    <id>http://devex.github.io/blog/2015/02/16/ubuntu-eol-upgrades</id>
    <content type="html"><![CDATA[<p>Have you ever updated your servers?
I trust most of you have.
Anyway, from time to time one finds an unsupported release, running on some forgotten server.
In some cases, it&rsquo;s an unused service or something to be decommissioned, but sometimes, we should update it.
At <a href="https://www.devex.com">Devex</a>, found this situation recently.
You&rsquo;ll find in this article how we upgraded some cases we couldn&rsquo;t decommission.</p>

<!-- more -->


<h2>Common aspects</h2>

<p>Our infrastructure is running on AWS EC2, so we take a lot of profit from the ability to create AMI images from running instances.
With this approach, we could use those images to try out the upgrade without affecting the instances.
Of course, this implies that data might be outdated, which can be fixed easily by reloading a backup from the correct instance, or by using the same procedure on the running instance later.</p>

<p>Also, the usage of ephemeral drive mounted on <code>/mnt</code> is a good place to store backups and files we would need to save during the process.</p>

<h2>Upgrading from Intrepid</h2>

<p>Our first case was upgrade an Intrepid Ubuntu server, running Postgres 8.3, to Lucid.
We chose to stay in Lucid because this application is going to be decommissioned soon, anyway.
However, this decommission won&rsquo;t happen immediately, since these features must be implemented in newer applications.
So, the first step was getting a backup from the PostgreSQL database, running these commands:</p>

<pre><code>sudo mkdir -p /mnt/postgresql
sudo chown -R postgres:postgres /mnt/postgresql
sudo -u postgres pg_dumpall &gt; /mnt/postgresql/backup
</code></pre>

<p>Once the backup was saved, we upgraded intrepid packages.
Of course the original Intrepid package repositories are down already, but we could take use of the <code>old-release</code> repositories.
To select them, we used the following commands:</p>

<pre><code>sudo cat &gt;/etc/apt/sources.list &lt;&lt;EOL
deb http://old-releases.ubuntu.com/ubuntu/ intrepid main restricted universe multiverse
deb http://old-releases.ubuntu.com/ubuntu/ intrepid-updates main restricted universe multiverse
deb http://old-releases.ubuntu.com/ubuntu/ intrepid-security main restricted universe multiverse
deb http://old-releases.ubuntu.com/ubuntu/ intrepid-backports main restricted universe multiverse
deb http://old-releases.ubuntu.com/ubuntu/ intrepid-proposed main restricted universe multiverse
EOL
sudo apt-get update
sudo apt-get upgrade
</code></pre>

<p>With these commands, we got an updated Intrepid release, so our following step was upgrade to Jaunty, by issuing the following commands:</p>

<pre><code>sudo perl -p -i.intrepid -e 's/intrepid/jaunty/' /etc/apt/sources.list
sudo apt-get update
sudo apt-get install update-manager-core
sudo apt-get upgrade
sudo reboot
</code></pre>

<p>After the reboot, we got a Jaunty, so next step is upgrading to Karmic, and Lucid:</p>

<pre><code>sudo perl -p -i.jaunty -e 's/jaunty/karmic/' /etc/apt/sources.list
sudo apt-get update
sudo apt-get upgrade
sudo do-release-upgrade -f DistUpgradeViewNonInteractive
sudo reboot
</code></pre>

<p>Karmic was the first release on which we could use <code>do-release-upgrade</code>.
In that case, we used the <code>-f DistUpgradeViewNonInteractive</code> option to have the release upgrade done without user intervention.
After that reboot, the OpenSSH server upgrade updated the host fingerprints, so we needed to change our <code>.ssh/known_hosts</code> removing the old one, before reconnecting.
Then, the final step was resetting the PostgreSQL cluster, issuing the following commands as the <code>postgres</code> user:</p>

<pre><code>pg_dropcluster --stop 8.4 main
pg_createcluster -u postgres -d /data/postgresql/8.4/main 8.4 main
service postgresql-8.4 start
psql -f /data/postgresql/backup postgres
</code></pre>

<p>Finally, after checking everything is in place, we just could remove the old cluster and setup:</p>

<pre><code>sudo rm -rf /data/postgresql/8.3 /etc/postgresql/8.3
</code></pre>

<h2>Upgrading from Maverick</h2>

<p>In that case we could upgrade to Precise, since Trusty just broke the boot process.
The first step was pretty similar, since Maverick package repositories are also outdated:</p>

<pre><code>sudo bash -c "cat &gt;/etc/apt/sources.list &lt;&lt;EOF
deb http://old-releases.ubuntu.com/ubuntu/ maverick main restricted universe multiverse
deb http://old-releases.ubuntu.com/ubuntu/ maverick-updates main restricted universe multiverse
deb http://old-releases.ubuntu.com/ubuntu/ maverick-security main restricted universe multiverse
deb http://old-releases.ubuntu.com/ubuntu/ maverick-backports main restricted universe multiverse
deb http://old-releases.ubuntu.com/ubuntu/ maverick-proposed main restricted universe multiverse
EOF
"
sudo apt-get update
sudo apt-get upgrade
sudo do-release-upgrade -f DistUpgradeViewNonInteractive
sudo reboot
</code></pre>

<p>After the reboot, we got a Natty release running, so updating to Oneiric was the next step. Pretty much the same:</p>

<pre><code>sudo do-release-upgrade -f DistUpgradeViewNonInteractive
sudo reboot
</code></pre>

<p>In our case, the application running here was a little bit sensible to automatic removal of packages for Precise upgrade.
So in the following step, we couldn&rsquo;t use the unattended mode of <code>do-release-upgrade</code>:</p>

<pre><code>sudo do-release-upgrade
</code></pre>

<p>When manually running the <code>do-release-upgrade</code>, we just ommitted the old package removal part.
After the reboot, issued from the <code>do-release-upgrade</code>, we got a working Precise release running.</p>

<h2>Details</h2>

<p>In first place, we&rsquo;d like to point the great importance of having backups and images for the instances.
With the backups, preferably dumps, you&rsquo;ll be able to recover the data for your databases.
Of course, if the application running on the affected instances are not in the same server, then you&rsquo;ll need to face the client libraries updating and possible version differences that might appear.
Here&rsquo;s where having the server images might help.
We took static images for having the servers upgrade procedures ready.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Performance Issues With Rails and VirtualBox]]></title>
    <link href="http://devex.github.io/blog/2015/02/06/performance-issues-with-rails-and-virtualbox/"/>
    <updated>2015-02-06T06:02:50-05:00</updated>
    <id>http://devex.github.io/blog/2015/02/06/performance-issues-with-rails-and-virtualbox</id>
    <content type="html"><![CDATA[<p>Two weeks ago, we noticed some performance issues with Rails in our development setup, while all our other environments, some much less powerful, were working with much better performance.
After confirming that no recent change caused this slow-down, and running some diagnostics and measurements to record the performance in some point, this took us on a small trip into some Ruby on Rails debugging on a VirtualBox.</p>

<!-- more -->


<h2>Our development setup</h2>

<p>Most of us at Devex, we use a specific setup of our apps into a <a href="https://www.virtualbox.org">VirtualBox</a> machine so we can hold a local development version of our site and check how the new features integrate before to give the work as done.
In some cases, for feature availability, performance, and workstation power reasons, these apps, and their components run completely on this environment, but in other cases, we don&rsquo;t run all the components in the virtual machine and we do use some components from the staging (we call it develop) environment, which is quite approximate to what we need.
So this development environment&rsquo;s architecture and infrastructure is quite different from the ones we have in staging, pre-production, and, of course, production.</p>

<p>Not only that, in all our other environments, we run the apps using <a href="http://unicorn.bogomips.org/">Unicorn</a> as a daemon, with more or less workers, while in this development environment, we run <a href="http://www.ruby-doc.org/stdlib-1.9.3/libdoc/webrick/rdoc/WEBrick.html">WEBrick</a> within a screen session, just to simplify the load.</p>

<h2>The problem arises</h2>

<p>The problem arose when we noticed Rails was underperforming when running in the local development setup.
Some measurements were taken showing up that the problem seemed to be located in our front-end application, since the back-end was running properly:</p>

<pre><code>$ time wget -pq --no-cache --delete-after http://localhost:3002/apps/front_end/api/system/health
real    0m0.523s
user    0m0.000s
sys 0m0.000s

$ time wget -pq --no-cache --delete-after http://localhost:3004/public/system/health
real    0m0.019s
user    0m0.000s
sys 0m0.000s
</code></pre>

<p>These two URIs produce more or less identical content, JSON-formatted status info regarding our front-end and back-end applications, respectively.
So, another measurement was done, which consisted into measuring the time to serve one complete page:</p>

<pre><code>$ time wget -pq --no-cache --delete-after http://localhost:3002/people
real    0m13.393s
user    0m0.008s
sys 0m0.032s
</code></pre>

<p>Looking for similar cases on Internet, we found some links related to <a href="http://stackoverflow.com/questions/1156759/webrick-is-very-slow-to-respond-how-to-speed-it-up">WEBrick performance improvements</a>, <a href="http://stackoverflow.com/questions/8670080/rails-3-1-on-ubuntu-11-10-under-virtualbox-very-slow">Rails performance within VirtualBox</a>, and <a href="http://www.visionfactory.com.au/blog/rails_dev_with_webrick_really_slow_in_a_">WEBrick reverse lookups</a>.</p>

<p>None of the solutions we tried from these links helped to find the solution.
We then tried doubling processors and memory available in the VM configuration, but it didn&rsquo;t solved the issue.
Measuring with <code>atop</code> showed no bottleneck.
Reproducibility of the issue in other computers and setups was also checked, so it was not something related to the specific setup or the hardware on which it was relying.</p>

<h2>What&rsquo;s going on then?</h2>

<p>Then, we started analizing the issue in some more depth, first taking a look on what was going on when the browser issued a request.
So, using the Network view in the web inspector, we saw very long waiting times for static content, but the rest of the times were ok, no execution issues or so.</p>

<p>Next step was taking a closer look to the log to see what would be happening when Rails received the static requests.
We could confirm that the logs were written much more slowly when serving static files. So we decided to trace the WEBrick process.</p>

<p>When we <code>strace</code>&rsquo;d the WEBrick process, we couldn&rsquo;t see anything meaningful, but then we learned WEBrick is threaded, so the actual requests where being attended by threads, which traces were not showed in the <code>strace</code> output.</p>

<p>The trick to <a href="http://superuser.com/questions/80556/how-do-you-view-all-threads-running-on-linux">identify the thread TIDs</a> is to run the <code>ps</code> command with the <code>-T</code> option, which lists also the threads as processes, with the corresponding id.
Then you can <a href="http://stackoverflow.com/questions/7698209/tracing-pthreads-in-linux">run <code>strace</code> on those TIDs</a>.
Finally we started to see errors on unavailable resources.
We searched again and <a href="http://mitchellh.com/comparing-filesystem-performance-in-virtual-machines">found out</a> that VirtualBox uses a specific filesystem for the shared folders, with some performance problems, and that NFS is one of the fastest ones you can use.</p>

<h2>The fix</h2>

<p>Some of us, simply copied the shared folder content to be used to a regular directory in the VM disk.
This fixes the issue, but introduces the need to keep copying the content when it gets updated.</p>

<p>Some others of us, decided to go for NFS, which has a couple of drawbacks:</p>

<ul>
<li>It is not compliant with having a Windows host, but this is not our case.</li>
<li><p>It requires a LAN to be setup between the host and the guest, but we solved the whole problem by adding the following to our Vagrantfile:</p>

<pre><code>ip = ENV['VMSETUP_IP'] || `vboxmanage list hostonlyifs | grep IPAddress | cut -d: -f2 | tr -d ' '`.to_s.tr("\n", "") + '0'
[...]
Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|
  [...]
  config.vm.network :private_network, ip: "#{ip}"
  [...]
  config.vm.synced_folder ".", "/vagrant", type: "nfs"
  [...]
end
</code></pre></li>
</ul>


<p>By the way, the final measurements are as follows:</p>

<pre><code>$  time wget -pq --no-cache --delete-after http://localhost:3004/public/system/health

real 0m0.016s
user 0m0.000s
sys 0m0.004s

$ time wget -pq --no-cache --delete-after http://localhost:3002/apps/front_end/api/system/health

real 0m0.066s
user 0m0.000s
sys 0m0.006s

$ time wget -pq --no-cache --delete-after http://localhost:3002/people

real 0m1.661s
user 0m0.012s
sys 0m0.026s
</code></pre>
]]></content>
  </entry>
  
</feed>
